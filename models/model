# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FYcHloCOJnHryxtjHmCMwi0uM6N43nD4
"""

import os
path = "/content/drive/MyDrive/Colab Notebooks"
os.chdir(path)

import os
from PIL import Image
import tensorflow as tf
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import logging

def dataload(filepath,labelpath):
  imgset = []
  data = pd.read_csv(labelpath)
  len = data["Image name"].shape[0]
  # label
  labelset =np.asarray (data["Retinopathy grade"])
  labelset[labelset<2] = 0
  labelset[labelset>=2] = 1
  #image
  for i in range(0,len):
    name = data["Image name"][i]
    impath = os.path.join(filepath,name+'.jpg')
    #image processing
    # img = Image.open(impath)
    # cropim = img.crop((200,0,3750,2848))
    # img = cropim.resize((256,256))
    # img = np.asarray(img)
    # tf image processing 
    img = tf.io.read_file(impath)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.convert_image_dtype(img, tf.float32)
    img = tf.image.crop_to_bounding_box(img,0,200,2848,3550)
    img = tf.image.pad_to_bounding_box(img, offset_height=400, offset_width=0, target_height=3550, target_width=3550)
    img = tf.image.resize(img,[256,256])
    imgset.append(img)
  dataset = tf.data.Dataset.from_tensor_slices((imgset, labelset))
  return dataset

def datasplit(dataset,buffer,seed,train_per):
  dataset1 = dataset.shuffle(buffer,seed = seed,reshuffle_each_iteration=False)
  num = sum(1 for _ in dataset1)
  train_dataset = dataset1.take(round(num*train_per))
  val_dataset = dataset1.skip(round(num*train_per)).take(round(num*(1-train_per)))
  return train_dataset, val_dataset

def datadis(dataset):
  i0 =0
  i1 =0
  for img,label in dataset:
    if label ==0:
      i0 +=1
    else:
      i1+=1
  num = sum(1 for _ in dataset)
  dis = [i0,i1,num]
  return dis

def get_data(trainimpath,testimpath,trainlabelpath,testlabelpath,train_per):
  fulldataset = dataload(trainimpath,trainlabelpath)
  testdataset = dataload(testimpath,testlabelpath)
  traindataset,valdataset = datasplit(fulldataset,500,2,0.9)
  traindis = datadis(traindataset)
  testdis = datadis(testdataset)
  valdis = datadis(valdataset)
  data_info = {'num_0':{'train':traindis[0],'test':testdis[0],'val':valdis[0]},
               'num_1':{'train':traindis[1],'test':testdis[1],'val':valdis[1]},
               'len':{'train':traindis[2],'test':testdis[2],'val':valdis[2]}}
  return traindataset,testdataset,valdataset,data_info

trainds,testds,valds,info = get_data("./IDRID_dataset/images/train","./IDRID_dataset/images/test","./IDRID_dataset/labels/train.csv","./IDRID_dataset/labels/test.csv",0.9)

#新加的
trainds = trainds.repeat(-1)
trainds = trainds.batch(32)
valds = valds.batch(1)

#Data augmentation
def augment(image, label):
    """Data augmentation"""
    image=tf.image.random_flip_left_right(image)
    image=tf.image.random_flip_up_down(image)
    return image, label
trainds = trainds.map(augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)

# i = 0
# for img,label in valds:
#   if i ==5:
#        print(label)
#        plt.imshow(img)
#        plt.show()
#        break
#   i +=1

#改成你的model
import gin
import tensorflow as tf

def vgg_block(inputs, channel, kernel_size, num_layer):
    out = tf.keras.layers.Conv2D(channel, kernel_size, padding='same', activation=tf.nn.relu)(inputs)
    num = num_layer-1
    for i in range(num):
        out = tf.keras.layers.Conv2D(channel, kernel_size, padding='same', activation=tf.nn.relu)(out)
    out = tf.keras.layers.MaxPool2D((2, 2))(out)

    return out
def vgg_like(input_shape, arch, n_classes, dense_units, dropout_rate):
    inputs = tf.keras.Input(shape=(256, 256, 3))
    out = vgg_block(inputs, arch[0][2], arch[0][1], arch[0][0])
    for (num_layer, kernel_size, channel) in arch[1:]:
        out = vgg_block(out, channel, kernel_size, num_layer)
    out = tf.keras.layers.GlobalAveragePooling2D()(out)
    out = tf.keras.layers.Dense(dense_units, activation=tf.nn.relu)(out)
    out = tf.keras.layers.Dropout(dropout_rate)(out)
    outputs = tf.keras.layers.Dense(n_classes)(out)

    return tf.keras.Model(inputs=inputs, outputs=outputs, name='vgg_like')

#导入你的model
arch = ((2, 3, 16), (2, 3, 32), (2, 3, 64), (2, 3, 128))
print(arch[1:])
input = (256, 256, 3)
model1 = vgg_like(input, arch, 2, 64,0.5)

class Trainer(object):
    def __init__(self, model, ds_train, ds_val, ds_info, total_steps, log_interval):
        # Summary Writer
        # ....

        # Checkpoint Manager
        # ...

        # Loss objective
        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
        self.optimizer = tf.keras.optimizers.Adam()

        # Metrics
        self.train_loss = tf.keras.metrics.Mean(name='train_loss')
        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

        self.val_loss = tf.keras.metrics.Mean(name='val_loss')
        self.val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')

        self.model = model
        self.ds_train = ds_train
        self.ds_val = ds_val
        self.ds_info = ds_info
        # self.run_paths = run_paths
        self.total_steps = total_steps
        self.log_interval = log_interval
        # self.ckpt_interval = ckpt_interval

    @tf.function
    def train_step(self, images, labels):
        with tf.GradientTape() as tape:
            # training=True is only needed if there are layers with different
            # behavior during training versus inference (e.g. Dropout).
            predictions = self.model(images, training=True)
            loss = self.loss_object(labels, predictions)
        gradients = tape.gradient(loss, self.model.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))

        self.train_loss(loss)
        self.train_accuracy(labels, predictions)

    @tf.function
    def val_step(self, images, labels):
        # training=False is only needed if there are layers with different
        # behavior during training versus inference (e.g. Dropout).
        predictions = self.model(images, training=False)
        t_loss = self.loss_object(labels, predictions)

        self.val_loss(t_loss)
        self.val_accuracy(labels, predictions)

    def train(self):
        for idx, (images, labels) in enumerate(self.ds_train):

            step = idx + 1
            self.train_step(images, labels)

            if step % self.log_interval == 0:

                # Reset test metrics
                self.val_loss.reset_states()
                self.val_accuracy.reset_states()

                for val_images, val_labels in self.ds_val:
                    self.val_step(val_images, val_labels)

                template = 'Step {}, Loss: {}, Accuracy: {}, Validation Loss: {}, Validation Accuracy: {}'
                logging.info(template.format(step,
                                             self.train_loss.result(),
                                             self.train_accuracy.result() * 100,
                                             self.val_loss.result(),
                                             self.val_accuracy.result() * 100))
                
                # Write summary to tensorboard
                # ...

                # Reset train metrics
                self.train_loss.reset_states()
                self.train_accuracy.reset_states()

                yield self.val_accuracy.result().numpy()

            # if step % self.ckpt_interval == 0:
            #     logging.info(f'Saving checkpoint to {self.run_paths["path_ckpts_train"]}.')
            #     # Save checkpoint
            #     # ...

            if step % self.total_steps == 0:
                logging.info(f'Finished training after {step} steps.')
                # Save final checkpoint
                # ...
                return self.val_accuracy.result().numpy()

trainer = Trainer(model1, trainds, valds, info, 10000,100)
for _ in trainer.train():
    continue

# logging.getLogger().setLevel(logging.INFO)
# logging.info('1')

# class TT():
#   def __init__(self,a, t):
#     self.step = a
#     self.tstep = t
#   def tr(self):
#     for idx  in range(0,100):
#       step = idx + 1
#       logging.info(idx)
#       if step % self.step == 0:
#         logging.info(f'after {step} steps.')
#         yield step
#       if step % self.tstep == 0:
#         logging.info(f'Finished training after {step} steps.')
#         return step
